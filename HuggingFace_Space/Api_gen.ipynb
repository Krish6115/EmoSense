{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":611742,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":459525,"modelId":475398}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# BLOCK 1: INSTALL LIBRARIES\n# ==============================================================================\n!pip install fastapi uvicorn pyngrok pydantic torch transformers emoji -q\nprint(\"âœ… Libraries installed.\")\n\n# ==============================================================================\n# BLOCK 2: THE API SERVER CODE\n# ==============================================================================\nimport uvicorn\nimport re\nimport emoji\nimport torch\nimport torch.nn.functional as F\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom torch.nn import Module, Linear\nfrom pyngrok import ngrok\nimport threading\nimport os\n# This is the NEW part for using Kaggle Secrets\nfrom kaggle_secrets import UserSecretsClient\n\n# --- Define the request body for the API ---\nclass Tweet(BaseModel):\n    text: str\n\n# --- Preprocessing Function (must be identical to the one used for training) ---\ndef preprocess_text(text: str):\n    if not isinstance(text, str): return \"\"\n    text = emoji.demojize(text)\n    text = text.lower()\n    text = re.sub(r'@\\w+', '', text)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'[^a-z\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# --- Re-define the Model Architecture (must be identical to your trained model) ---\nclass EmoRoBERTa(Module):\n    def __init__(self, n_emotions):\n        super(EmoRoBERTa, self).__init__()\n        self.roberta = RobertaModel.from_pretrained('roberta-base')\n        self.emotion_classifier = Linear(self.roberta.config.hidden_size, n_emotions)\n        self.intensity_regressor = Linear(self.roberta.config.hidden_size, n_emotions)\n        self.sarcasm_detector = Linear(self.roberta.config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n        return (self.emotion_classifier(outputs), self.intensity_regressor(outputs), self.sarcasm_detector(outputs))\n\n# --- Load Model, Tokenizer, and Configuration ---\nmodel = None\ntry:\n    # ** FIX #1: PASTE YOUR CORRECT FILE PATH HERE **\n    MODEL_PATH = '/kaggle/input/emoberta_model/pytorch/default/1/emoberta_model.bin' # <-- PASTE CORRECT PATH\n\n    # This list is CORRECT because it will have the 12 emotions from your original training\n    EMOTION_CLASSES = ['anger', 'annoyance', 'approval', 'caring', 'curiosity', 'fear', 'gratitude', 'joy', 'love', 'neutral', 'sadness', 'surprise'] # <-- PASTE YOUR REAL LIST HERE\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    model = EmoRoBERTa(n_emotions=len(EMOTION_CLASSES)).to(device)\n    \n    if os.path.exists(MODEL_PATH):\n        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n        model.eval()\n        print(f\"âœ… Model loaded successfully from {MODEL_PATH}!\")\n    else:\n        print(f\"âŒ ERROR: Model file not found at {MODEL_PATH}\")\n        model = None\n        \nexcept Exception as e:\n    print(f\"--- âŒ An error occurred while loading the model: {e} ---\")\n    model = None\n\n# --- Initialize the FastAPI app ---\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"status\": \"EmoSense API is running\"}\n\n@app.post(\"/analyze\")\ndef analyze_emotion(tweet: Tweet):\n    if not model:\n        return {\"error\": \"Model is not loaded. Please check the logs.\"}\n    # ... (rest of the function is the same)\n    text = tweet.text\n    cleaned_text = preprocess_text(text)\n    encoding = tokenizer.encode_plus(\n        cleaned_text, add_special_tokens=True, max_length=128, return_token_type_ids=False,\n        padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt'\n    )\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    with torch.no_grad():\n        emotion_logits, intensity_scores, sarcasm_logits = model(input_ids, attention_mask)\n        emotion_probs = F.sigmoid(emotion_logits).flatten().cpu().numpy()\n        sarcasm_prob = F.sigmoid(sarcasm_logits).flatten().cpu().numpy()[0]\n        intensity_scores = intensity_scores.flatten().cpu().numpy()\n    return {\n        \"text\": text,\n        \"emotions_confidence\": {emotion: float(prob) for emotion, prob in zip(EMOTION_CLASSES, emotion_probs)},\n        \"predicted_intensity\": {emotion: float(score) for emotion, score in zip(EMOTION_CLASSES, intensity_scores)},\n        \"sarcasm_score\": float(sarcasm_prob)\n    }\n    \n# ==============================================================================\n# BLOCK 3: LAUNCH THE SERVER AND GET YOUR PUBLIC URL\n# ==============================================================================\n# ** FIX #2: ADDED CODE TO USE YOUR KAGGLE SECRET **\ntry:\n    # This line securely gets your authtoken from the Kaggle Secret you created\n    ngrok.set_auth_token(UserSecretsClient().get_secret(\"NGROK_AUTH_TOKEN\"))\n    print(\"âœ… ngrok token authenticated successfully.\")\nexcept Exception as e:\n    print(f\"âŒ Could not authenticate ngrok. Error: {e}\")\n    print(\"Please ensure you've added a Kaggle Secret with the label 'NGROK_AUTH_TOKEN'.\")\n\n# Run the app and get the public URL\npublic_url = ngrok.connect(8000)\nprint(f\"ðŸš€ EmoSense API is live! Your public URL is: {public_url}\")\n\n# Start the server\ndef run_server():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nthread = threading.Thread(target=run_server)\nthread.start()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-18T06:11:58.919708Z","iopub.execute_input":"2025-10-18T06:11:58.919913Z","iopub.status.idle":"2025-10-18T06:14:22.105647Z","shell.execute_reply.started":"2025-10-18T06:11:58.919893Z","shell.execute_reply":"2025-10-18T06:14:22.096626Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mâœ… Libraries installed.\n","output_type":"stream"},{"name":"stderr","text":"2025-10-18 06:13:55.636200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760768035.863022      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760768035.921227      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3267cf277bf4e109c3c07be5471879e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04e5ab4493834a22bdcf424eddb72092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc2a21bcff2b43c4936d6a53a4d00980"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e94c9e51ec4166b9452aefa887c75d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bd1aa6c00784ff2ab901346d14c9677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5b67c7d28cb4aa99b14d9b84e463d24"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"âœ… Model loaded successfully from /kaggle/input/emoberta_model/pytorch/default/1/emoberta_model.bin!\nâœ… ngrok token authenticated successfully.                                                           \nðŸš€ EmoSense API is live! Your public URL is: NgrokTunnel: \"https://exogenously-connectable-li.ngrok-free.dev\" -> \"http://localhost:8000\"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}